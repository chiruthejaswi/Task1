{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiruthejaswi/Task1/blob/main/task1_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4i3sp07G-ZV"
      },
      "source": [
        "# **Neural Network from Scratch: Income Classification Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3EUmadRHEq6"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "Predict whether a person earns more than $50,000 per year using a feedforward neural network built from scratch in PyTorch on the UCI Adult Income dataset. This problem involves hanling tabular data with both categorical and numerical features, making it an ideal candidate for deep learning experimentation and ablation studies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxxjFlw34qU0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,LabelEncoder,MinMaxScaler,StandardScaler,RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "import numpy as np\n",
        "from torch.optim import SGD,Adam,RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install shap\n",
        "import shap\n",
        "shap.initjs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NEw2z9VDaTO"
      },
      "source": [
        "# **Loading Data:**\n",
        "\n",
        "We begin by loading the Adult Income dataset. It comes in two parts: adult.data (train) and adult.test (test). Both are loaded with consistent column names, and the test file skips the first line as it contains comments.\n",
        "\n",
        "We then combine both datasets into a single DataFrame for uniform preprocessing.\n",
        "\n",
        "\n",
        "here is the link for the data set:https://drive.google.com/drive/folders/1nW-dv41X-xHxz3qrh8-YnweRSp-lKG4I?usp=drive_link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEBs-EUVDbBD"
      },
      "outputs": [],
      "source": [
        "# load dataset and naming columns\n",
        "column_names=['age','workclass','fnlwgt','education','education-num','marital-status',\n",
        "              'occupation','relationship','race','sex','capital-gain','capital-loss',\n",
        "              'hours-per-week','native-country','income']\n",
        "\n",
        "# loading train and test datasets\n",
        "df_train =pd.read_csv(\"/content/drive/MyDrive/adult_income_dataset/adult.data\",header=None,names=column_names,na_values='?')\n",
        "df_test =pd.read_csv(\"/content/drive/MyDrive/adult_income_dataset/adult.test\",names=column_names,na_values='?',skiprows=1) # skipping one row because we have a comment line at its head\n",
        "\n",
        "# combining both for uniform preprocessing\n",
        "df=pd.concat([df_train,df_test])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPGzYJAn596K"
      },
      "source": [
        "# **Handling Missing Values**\n",
        "\n",
        "We identify and handle missing values using two strategies:\n",
        "\n",
        "* **Dropping Missing Values:**\n",
        "\n",
        "We use df.dropna() to remove rows with missing values. This leads to less data but avoids assumptions during imputation.\n",
        "\n",
        "* **Imputing Missing Values:**\n",
        "\n",
        "\n",
        "We apply imputation\n",
        "\n",
        "***For categorical columns:*** fill missing values with the most frequent (mode).\n",
        "\n",
        "***For numerical columns:*** fill missing values with the mean.\n",
        "\n",
        "This retains all rows in the dataset while providing reasonable estimates for missing entries.\n",
        "\n",
        "We now have two versions of the dataset:\n",
        "\n",
        "df_dropna: rows with missing values dropped\n",
        "\n",
        "df_imputed: missing values imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR6psod8Nihh"
      },
      "outputs": [],
      "source": [
        "# Handling missing values\n",
        "\n",
        "# Let's chcek the missing values first\n",
        "df.isna().sum()\n",
        "\n",
        "# 1.by dropping missing rows\n",
        "df_dropna=df.dropna()\n",
        "\n",
        "# 2.by imputing missing\n",
        "df_imputed = df.copy()\n",
        "# selecting categorical columns and numerical columns seperately\n",
        "cat_columns = df_imputed.select_dtypes(include='object').columns\n",
        "num_columns = df_imputed.select_dtypes(include='number').columns\n",
        "# imputed the categorical values with mode and numerical values with mean\n",
        "imputed_cat = SimpleImputer(strategy='most_frequent')\n",
        "imputed_num = SimpleImputer(strategy='mean')\n",
        "\n",
        "df_imputed[cat_columns] = imputed_cat.fit_transform(df_imputed[cat_columns])\n",
        "df_imputed[num_columns] = imputed_num.fit_transform(df_imputed[num_columns])\n",
        "df_imputed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqZ7eq4_62pp"
      },
      "source": [
        "# **Encoding Categorical Features**\n",
        "\n",
        "Categorical data must be encoded to numerical form for most ML models. We apply three different encoding techniques to both df_imputed and df_dropna.\n",
        "\n",
        "# Label Encoding\n",
        "Each unique category is converted to a unique integer. Suitable for tree-based models but may impose unintended order.\n",
        "\n",
        "# Ordinal Encoding\n",
        "Like label encoding, but applied with OrdinalEncoder() across all columns in one go.\n",
        "\n",
        "# One-Hot Encoding\n",
        "Each category becomes a separate binary column. This avoids false ordinal relationships but increases dimensionality.\n",
        "\n",
        "* We also ensure the target column income is excluded from encoding and handled separately.\n",
        "\n",
        "This results in six encoded datasets:\n",
        "\n",
        "{df_label_encoded1, df_ordinal_encoded1, df_onehot_encoded1 (from df_imputed)\n",
        "\n",
        "df_label_encoded2, df_ordinal_encoded2, df_onehot_encoded2 (from df_dropna)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TfXkHKPsLY2"
      },
      "source": [
        "**combination of imputed dataframe with OneHotEncoding,Ordinal Encoding,Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB9RUuA6ztWR"
      },
      "outputs": [],
      "source": [
        "# define categorical and numerical columns\n",
        "cat_columns= df_imputed.select_dtypes(include='object').columns\n",
        "num_columns =df_imputed.select_dtypes(include='number').columns\n",
        "\n",
        "# Label Encoding\n",
        "df_label_encoded1=df_imputed.copy()\n",
        "label_encoders={}\n",
        "\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_label_encoded1[col]= le.fit_transform(df_label_encoded1[col])\n",
        "    label_encoders[col] =le\n",
        "print(\"Label Encoding completed. Shape:\", df_label_encoded1.shape)\n",
        "\n",
        "# Ordinal Encoding\n",
        "df_ordinal_encoded1= df_imputed.copy()\n",
        "ordinal_encoder= OrdinalEncoder()\n",
        "df_ordinal_encoded1[cat_columns]= ordinal_encoder.fit_transform(df_ordinal_encoded1[cat_columns])\n",
        "\n",
        "print(\"Ordinal Encoding completed.Shape:\", df_ordinal_encoded1.shape)\n",
        "\n",
        "onehot_encoder= OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "onehot_encoded= onehot_encoder.fit_transform(df_imputed[cat_columns])\n",
        "onehot_feature_names =onehot_encoder.get_feature_names_out(cat_columns)\n",
        "\n",
        "# Combine with numerical columns\n",
        "df_onehot= pd.DataFrame(onehot_encoded, columns=onehot_feature_names)\n",
        "df_onehot_encoded1 =pd.concat([df_imputed[num_columns].reset_index(drop=True), df_onehot], axis=1)\n",
        "\n",
        "print(\"One-Hot Encoding done.Shape:\",df_onehot_encoded1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAJyCaSQsmcn"
      },
      "source": [
        "**Combination of dropped data frame with OneHotEncoding,Ordinal encoding,Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl9C0msms-fa"
      },
      "outputs": [],
      "source": [
        "# Label Encoding\n",
        "df_label_encoded2=df_dropna.copy()\n",
        "label_encoders={}\n",
        "\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_label_encoded2[col]= le.fit_transform(df_label_encoded2[col])\n",
        "    label_encoders[col] =le\n",
        "print(\"Label Encoding done. Shape:\", df_label_encoded2.shape)\n",
        "\n",
        "# Ordinal Encoding\n",
        "df_ordinal_encoded2 = df_dropna.copy()\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "df_ordinal_encoded2[cat_columns] = ordinal_encoder.fit_transform(df_ordinal_encoded2[cat_columns])\n",
        "\n",
        "print(\"Ordinal Encoding done. Shape:\", df_ordinal_encoded2.shape)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "onehot_encoded = onehot_encoder.fit_transform(df_dropna[cat_columns])\n",
        "onehot_feature_names = onehot_encoder.get_feature_names_out(cat_columns)\n",
        "\n",
        "# Combine with numerical columns\n",
        "df_onehot = pd.DataFrame(onehot_encoded, columns=onehot_feature_names)\n",
        "df_onehot_encoded2 = pd.concat([df_dropna[num_columns].reset_index(drop=True), df_onehot], axis=1)\n",
        "\n",
        "print(\"One-Hot Encoding done. Shape:\", df_onehot_encoded2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWpnQSC87Zkd"
      },
      "source": [
        "# **Feature Scaling**\n",
        "\n",
        "To bring numerical features into similar ranges and improve model convergence, we apply three different scalers:\n",
        "\n",
        "**MinMaxScaler:** Scales to [0, 1] range.\n",
        "\n",
        "**StandardScaler:** Standardizes with mean = 0 and std = 1.\n",
        "\n",
        "**RobustScaler:** Uses median and IQR; better for outliers.\n",
        "\n",
        "We apply each scaler to each of the 6 encoded datasets, resulting in 18 total scaled feature sets.\n",
        "\n",
        "These are stored in a dictionary scaled_dfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJsO_5uOZi-o"
      },
      "outputs": [],
      "source": [
        "# list of scalers\n",
        "scalers= {\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'RobustScaler': RobustScaler()}\n",
        "\n",
        "# list of data frames\n",
        "encoded_dfs= {\n",
        "    'label_imputed': df_label_encoded1,\n",
        "    'label_dropna': df_label_encoded2,\n",
        "    'ordinal_imputed': df_ordinal_encoded1,\n",
        "    'ordinal_dropna': df_ordinal_encoded2,\n",
        "    'onehot_imputed': df_onehot_encoded1,\n",
        "    'onehot_dropna': df_onehot_encoded2}\n",
        "# Dictionary to store final scaled dataframes\n",
        "scaled_dfs={}\n",
        "\n",
        "# applying scalers for each encoded data frame\n",
        "for name,df in encoded_dfs.items():\n",
        "    # Identify numeric columns\n",
        "    numeric_cols=df.select_dtypes(include='number').columns\n",
        "\n",
        "    for scaler_name,scaler in scalers.items():\n",
        "        df_copy= df.copy()\n",
        "        df_copy[numeric_cols] =scaler.fit_transform(df_copy[numeric_cols])\n",
        "\n",
        "        combo_name= f\"{name}_{scaler_name}\"\n",
        "        scaled_dfs[combo_name]= df_copy\n",
        "        print(f\"{combo_name} done.Shape: {df_copy.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7dtIK417yzD"
      },
      "source": [
        "# **Train/Validation/Test split**\n",
        "For each of the 18 scaled datasets, we perform a 70/15/15 split:\n",
        "\n",
        "Split into 70% train and 30% temp.\n",
        "\n",
        "Split the temp set equally into validation and test (15% each).\n",
        "\n",
        "This ensures robust model evaluation.\n",
        "\n",
        "We handle the target column differently based on encoding:\n",
        "\n",
        "For label/ordinal encoded datasets, y = df['income']\n",
        "\n",
        "For one-hot encoded datasets, y = df[['income_ <=50K', 'income_ >50K']] and all income columns are excluded from X.\n",
        "\n",
        "The splits are stored in a dictionary splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoD_NdrUvZrV"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store splits\n",
        "splits={}\n",
        "\n",
        "for name,df in scaled_dfs.items():\n",
        "    if 'onehot' in name:\n",
        "        # For one-hot encoded dataframes, drop all 'income' related columns\n",
        "        income_cols=[col for col in df.columns if 'income' in col]\n",
        "        X= df.drop(columns=income_cols)\n",
        "        # Assuming the last two income columns are the target for one-hot encoded data\n",
        "        y = df[['income_ <=50K', 'income_ >50K']]\n",
        "    else:\n",
        "        # For Label and Ordinal encoded dataframes, drop the 'income' column\n",
        "        X = df.drop('income', axis=1)\n",
        "        y = df['income']\n",
        "\n",
        "    # First split: Train (70%) and Temp (30%)\n",
        "    X_train,X_temp,y_train,y_temp =train_test_split(X,y,test_size=0.30,random_state=42,stratify=y if not isinstance(y,pd.DataFrame) else y.iloc[:,0])\n",
        "\n",
        "    # Second split: Validation (15%) and Test (15%) from the 30%\n",
        "    X_val,X_test,y_val,y_test=train_test_split(X_temp, y_temp,test_size=0.50,random_state=42,stratify=y_temp if not isinstance(y_temp, pd.DataFrame) else y_temp.iloc[:, 0])\n",
        "\n",
        "\n",
        "    splits[name] = {\n",
        "        'X_train': X_train,\n",
        "        'y_train': y_train,\n",
        "        'X_val': X_val,\n",
        "        'y_val': y_val,\n",
        "        'X_test': X_test,\n",
        "        'y_test': y_test}\n",
        "\n",
        "    print(f\"Split done for {name} — Train:{X_train.shape},Val:{X_val.shape},Test:{X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbSRIQaXD8vk"
      },
      "source": [
        "Now that our dataset has been fully preprocessed —> encoded, scaled, and\n",
        "split.\n",
        "\n",
        "In this section, we'll implement a feedforward neural network using PyTorch.Our goal is to classify whether a person earns more than $50K per year based on a range of demographic and work-related features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElPExs5uNhZ2"
      },
      "source": [
        "# **2.Model Building and Architecture Ablation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VRg8-gRSU9"
      },
      "source": [
        "This section investigates the effect of varying the network architecture (depth, width), activation functions, and the inclusion of regularization layers like dropout and batch normalization. All models were trained and evaluated across all 18 dataset preprocessing combinations to measure the robustness and generalization power of each architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmBFUq_LRcPv"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "To explore how the structure of a feedforward neural network affects classification performance. Specifically, we test:\n",
        "\n",
        "* Shallow vs deep networks\n",
        "\n",
        "* ReLU vs Tanh vs LeakyReLU\n",
        "\n",
        "* Dropout for regularization\n",
        "\n",
        "* BatchNorm for training stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaURu0PpRsJb"
      },
      "source": [
        "**Model Implementation**\n",
        "\n",
        "We implemented a modular class FeedforwardNN with the following properties:\n",
        "\n",
        "* Accepts arbitrary number of hidden layers (hidden_dims)\n",
        "\n",
        "* Activations are passed as constructor arguments (nn.ReLU, nn.Tanh, nn.LeakyReLU)\n",
        "\n",
        "* Optional dropout and batch normalization\n",
        "\n",
        "* Output layer is always nn.Linear → Sigmoid (for BCELoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0X6JptmSEv0"
      },
      "source": [
        "Each model was trained using:\n",
        "\n",
        "* Optimizer: Adam (lr = 0.001)\n",
        "\n",
        "* Loss: BCELoss\n",
        "\n",
        "* Batch size: 64\n",
        "\n",
        "* Early stopping with patience = 3\n",
        "\n",
        "* Max epochs: 30\n",
        "\n",
        "* Trained on all 18 preprocessing pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLMsgyiUScYR"
      },
      "source": [
        "**Training Strategy**\n",
        "\n",
        "The training was done using a train_model() function with early stopping that monitored F1 score on validation set. For each dataset-architecture pair:\n",
        "\n",
        "* Training loss was tracked over epochs\n",
        "\n",
        "* Best model (based on F1) was saved and evaluated on the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoh99LrtStj1"
      },
      "source": [
        "**Results Recorded**\n",
        "\n",
        "For each dataset + architecture combination:\n",
        "\n",
        "* Final validation accuracy\n",
        "\n",
        "* Final validation F1 score\n",
        "\n",
        "* Per-epoch loss (stored in loss_curves)\n",
        "\n",
        "* Saved into arch_results_df → exported as arch_results.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmJDJI4TS9I9"
      },
      "source": [
        " **Observations**\n",
        "\n",
        "* Model A4 (deep, ReLU + Dropout + BatchNorm) consistently performed best across most datasets\n",
        "\n",
        "* Tanh activation worked well with dropout but converged slower\n",
        "\n",
        "* LeakyReLU + BatchNorm was stable but didn't outperform A4\n",
        "\n",
        "* Shallow networks (A1, A3) had faster convergence but poorer generalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yigtNw7uTTjA"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "* A4 ([128, 64, 32], ReLU, Dropout, BatchNorm) was selected as the best architecture\n",
        "\n",
        "* This architecture was used as the fixed baseline for Sections 3–6\n",
        "\n",
        "* Depth + normalization + dropout → best generalization performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr7fqcXjx80H"
      },
      "outputs": [],
      "source": [
        "#  Define a customizable Feedforward Neural Network\n",
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dims,activation_fn,use_dropout=False,use_batchnorm=False):\n",
        "        super(FeedforwardNN,self).__init__()\n",
        "        layers= []\n",
        "        in_dim=input_dim\n",
        "\n",
        "        # Loop over each hidden layer dimension\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(in_dim,hidden_dim))  # Fully connected layer\n",
        "\n",
        "            # Add batch normalization if enabled (helps training stability and speed)\n",
        "            if use_batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "            # Add activation (e.g., ReLU, Tanh, LeakyReLU)\n",
        "            layers.append(activation_fn())\n",
        "\n",
        "            # Add dropout if enabled (helps regularization, prevents overfitting)\n",
        "            if use_dropout:\n",
        "                layers.append(nn.Dropout(0.3))  # Fixed dropout rate for consistency\n",
        "\n",
        "            in_dim=hidden_dim  # Output of current layer becomes input to next\n",
        "\n",
        "        # Final output layer: 1 neuron with Sigmoid for binary classification\n",
        "        layers.append(nn.Linear(in_dim,1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.model=nn.Sequential(*layers)  # Combine all layers into one sequential block\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.model(x)  # Forward pass through the model\n",
        "# Architecture configurations to experiment with\n",
        "# Each config varies in depth, activation function, dropout, and batch normalization\n",
        "configs = [\n",
        "    {\"name\": \"2 layers (64, 32) - ReLU\", \"hidden_dims\": [64, 32], \"activation\": nn.ReLU, \"dropout\": False, \"batchnorm\": False},\n",
        "    {\"name\": \"3 layers (128, 64, 32) - Tanh+Dropout\", \"hidden_dims\": [128, 64, 32], \"activation\": nn.Tanh, \"dropout\": True, \"batchnorm\": False},\n",
        "    {\"name\": \"2 layers (64, 32) -LeakyReLU+BatchNorm\", \"hidden_dims\": [64, 32], \"activation\": nn.LeakyReLU, \"dropout\": False, \"batchnorm\": True},\n",
        "    {\"name\": \"3 layers (128, 64, 32)-ReLU+ Dropout + BatchNorm\", \"hidden_dims\": [128, 64, 32], \"activation\": nn.ReLU, \"dropout\": True, \"batchnorm\": True}\n",
        "]\n",
        "\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "arch_results= []     # To store (dataset, architecture, acc, f1)\n",
        "loss_curves= {}      # To track training loss per epoch\n",
        "\n",
        "# Function to evaluate a trained model on validation data\n",
        "def evaluate_model(model, X, y, device='cpu'):\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout/batchnorm updates)\n",
        "    with torch.no_grad(): # No gradients needed for evaluation\n",
        "        X = X.to(device).float()\n",
        "\n",
        "        # If labels are in DataFrame/Series format, convert to binary (0/1)\n",
        "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
        "            y = y.apply(lambda x: 1 if '>50K' in str(x) else 0).values\n",
        "        y = torch.tensor(y).to(device).int()\n",
        "\n",
        "        preds = model(X).view(-1)  # Flatten output\n",
        "        preds_cls = (preds > 0.5).int()  # Convert probabilities to 0/1\n",
        "\n",
        "        # Compute accuracy and F1 score\n",
        "        acc = accuracy_score(y.cpu(), preds_cls.cpu())\n",
        "        f1 = f1_score(y.cpu(), preds_cls.cpu())\n",
        "        return acc, f1\n",
        "\n",
        "# Training function with early stopping based on F1 score\n",
        "def train_model(model, train_loader, val_data, optimizer, criterion, patience=3, max_epochs=30):\n",
        "    X_val, y_val = val_data\n",
        "    best_f1 = 0               # Best F1 score seen so far\n",
        "    early_stop_count = 0      # How many epochs since improvement\n",
        "    best_state = None         # To store best model weights\n",
        "    losses = []               # Track training loss per epoch\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Iterate over mini-batches from training data\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).view(-1, 1).float()\n",
        "\n",
        "            optimizer.zero_grad()          # Clear gradients\n",
        "            out = model(xb).view(-1, 1)    # Forward pass\n",
        "            loss = criterion(out, yb)      # Compute loss\n",
        "            loss.backward()                # Backprop\n",
        "            optimizer.step()              # Update weights\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Evaluate on validation set after each epoch\n",
        "        acc, f1 = evaluate_model(model, X_val, y_val, device=device)\n",
        "        print(f\"Epoch {epoch+1:02d} - Loss: {avg_loss:.4f} | Val Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_state = model.state_dict()\n",
        "            early_stop_count = 0  # reset counter if improved\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Restore best model state\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, losses\n",
        "# Run all models on all 18 preprocessed datasets\n",
        "for dataset_name, split in splits.items():\n",
        "    print(f\"\\n Dataset: {dataset_name}\")\n",
        "\n",
        "    # Ensure label is Series, not DataFrame\n",
        "    y_train_series = split['y_train']\n",
        "    y_val_series = split['y_val']\n",
        "    if isinstance(y_train_series, pd.DataFrame):\n",
        "        y_train_series = y_train_series.iloc[:, 0]\n",
        "    if isinstance(y_val_series, pd.DataFrame):\n",
        "        y_val_series = y_val_series.iloc[:, 0]\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train= torch.tensor(split['X_train'].values).float()\n",
        "    y_train= torch.tensor(y_train_series.apply(lambda x: 1 if '>50K' in str(x) else 0).values).float()\n",
        "    X_val = torch.tensor(split['X_val'].values).float()\n",
        "    y_val = torch.tensor(y_val_series.apply(lambda x: 1 if '>50K' in str(x) else 0).values).int()\n",
        "\n",
        "    # Use DataLoader for mini-batch training\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "\n",
        "    # Try each architecture configuration\n",
        "    for cfg in configs:\n",
        "        print(f\"\\n Model: {cfg['name']}\")\n",
        "\n",
        "        # Instantiate the model with the given configuration\n",
        "        model = FeedforwardNN(\n",
        "            input_dim=X_train.shape[1],\n",
        "            hidden_dims=cfg['hidden_dims'],\n",
        "            activation_fn=cfg['activation'],\n",
        "            use_dropout=cfg['dropout'],\n",
        "            use_batchnorm=cfg['batchnorm']\n",
        "        ).to(device)\n",
        "\n",
        "        # Set optimizer and loss\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Train the model and evaluate\n",
        "        model, losses = train_model(model, train_loader, (X_val, y_val), optimizer, criterion)\n",
        "        acc, f1 = evaluate_model(model, X_val, y_val, device=device)\n",
        "\n",
        "        print(f\" Final Val Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "        # Save results\n",
        "        key = f\"{dataset_name} | {cfg['name']}\"\n",
        "        arch_results.append((dataset_name, cfg['name'], acc, f1))\n",
        "        loss_curves[key] = losses\n",
        "# Save the results to a CSV file for analysis and visualization\n",
        "arch_results_df = pd.DataFrame(arch_results, columns=[\"Dataset\", \"Architecture\", \"Val Accuracy\", \"F1 Score\"])\n",
        "arch_results_df.to_csv(\"arch_results.csv\", index=False)\n",
        "print(\"Saved to arch_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDfYpvRZH7ol"
      },
      "outputs": [],
      "source": [
        "# Example: visualize a few losses\n",
        "plt.figure(figsize=(12, 6))\n",
        "for name,losses in list(loss_curves.items())[:5]:  # plot first 5 configs\n",
        "    plt.plot(losses,label=name)\n",
        "plt.title(\"Training Loss Curves (first 5)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(fontsize=\"small\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curves_section2.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-gaehM78A9q"
      },
      "source": [
        "# **3.Loss Function and Optimizer Ablation**\n",
        "\n",
        "This section explores the impact of different loss functions, optimizers, and learning rates on training dynamics and model generalization. Understanding how these training hyperparameters interact is crucial for achieving fast convergence and high performance on validation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQnQgU7A8S2j"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "To identify the best combination of:\n",
        "\n",
        "* Loss Function:\n",
        "\n",
        "    1.BCELoss (requires Sigmoid in the model)\n",
        "\n",
        "    2.BCEWithLogitsLoss (more stable; no Sigmoid in model)\n",
        "\n",
        "* Optimizer:\n",
        "\n",
        "    * Adam (adaptive)\n",
        "\n",
        "    * SGD (vanilla)\n",
        "\n",
        "    * RMSprop (adaptive)\n",
        "\n",
        "* Learning Rate:\n",
        "\n",
        "    * 0.001 (safe)\n",
        "\n",
        "    * 0.01 (moderate)\n",
        "\n",
        "    * 0.1 (aggressive)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Q7ZZJCU8Dy"
      },
      "source": [
        "**Experimental Setup**\n",
        "\n",
        "* Architecture: Fixed as best from Section 2 → [128, 64, 32], ReLU, Dropout (0.3), BatchNorm\n",
        "\n",
        "* Dataset: Fixed as best preprocessed version → label_imputed_StandardScaler\n",
        "\n",
        "* Training:\n",
        "\n",
        "  * Optimizer and loss varied as per combinations\n",
        "\n",
        "  * Early stopping used (patience = 3)\n",
        "\n",
        "  * Epochs: max 30\n",
        "\n",
        "  * Batch Size: 64\n",
        "\n",
        "For BCEWithLogitsLoss, the final Sigmoid in the model is replaced with nn.Identity()\n",
        "Now we have total 18 experimental setups\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoZf9jlEVqVJ"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "* BCEWithLogitsLoss consistently outperformed BCELoss, likely due to improved numerical stability.\n",
        "\n",
        "* Adam was the most robust optimizer, performing well at all learning rates, especially 0.001.\n",
        "\n",
        "* SGD was sensitive to learning rate. At 0.01 it worked decently, but diverged at 0.1.\n",
        "\n",
        "* RMSprop performed better than SGD, but not better than Adam.\n",
        "\n",
        "* High learning rate (0.1) caused unstable training for SGD and RMSprop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBNi8j-DV8kk"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "* Best combination : BCEWithLogitsLoss + Adam + lr = 0.001\n",
        "\n",
        "* This combination was used in later sections (regularization, tuning, explainability)\n",
        "\n",
        "* BCEWithLogitsLoss is recommended when using logits directly (no Sigmoid in model)\n",
        "\n",
        "* Adam is a good default optimizer; it adapts well and avoids manual tuning pitfalls\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQdeD38hN_jN"
      },
      "outputs": [],
      "source": [
        "# Fixed config\n",
        "input_dim= splits['label_imputed_StandardScaler']['X_train'].shape[1]\n",
        "hidden_dims= [128, 64, 32]\n",
        "activation_fn= nn.ReLU\n",
        "use_dropout= True\n",
        "use_batchnorm= True\n",
        "batch_size= 64\n",
        "patience= 3\n",
        "max_epochs= 30\n",
        "\n",
        "# Combinations to test\n",
        "loss_functions= {\n",
        "    \"BCELoss\": nn.BCELoss(),\n",
        "    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss()\n",
        "}\n",
        "\n",
        "optimizers= [\"Adam\",\"SGD\",\"RMSprop\"]\n",
        "learning_rates= [0.001,0.01,0.1]\n",
        "\n",
        "# Prepare data (from best dataset)\n",
        "split= splits['label_imputed_StandardScaler']\n",
        "X_train= torch.tensor(split['X_train'].values).float()\n",
        "X_val =torch.tensor(split['X_val'].values).float()\n",
        "\n",
        "y_train_series= split['y_train']\n",
        "y_val_series= split['y_val']\n",
        "if isinstance(y_train_series,pd.DataFrame):\n",
        "    y_train_series= y_train_series.iloc[:, 0]\n",
        "if isinstance(y_val_series, pd.DataFrame):\n",
        "    y_val_series= y_val_series.iloc[:, 0]\n",
        "\n",
        "y_train= torch.tensor(y_train_series.apply(lambda x: 1 if x > 0 else 0).values).float()\n",
        "y_val= torch.tensor(y_val_series.apply(lambda x: 1 if x > 0 else 0).values).int()\n",
        "\n",
        "train_loader =DataLoader(TensorDataset(X_train,y_train),batch_size=batch_size,shuffle=True)\n",
        "\n",
        "# Results store\n",
        "opt_results=[]\n",
        "\n",
        "# Loop through combinations\n",
        "for loss_name,loss_fn in loss_functions.items():\n",
        "    for opt_name in optimizers:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"\\n {loss_name} + {opt_name} + lr={lr}\")\n",
        "\n",
        "            model = FeedforwardNN(\n",
        "                input_dim=input_dim,\n",
        "                hidden_dims=hidden_dims,\n",
        "                activation_fn=activation_fn,\n",
        "                use_dropout=use_dropout,\n",
        "                use_batchnorm=use_batchnorm\n",
        "            ).to(device)\n",
        "\n",
        "            # Choose optimizer\n",
        "            if opt_name== \"Adam\":\n",
        "                optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "            elif opt_name== \"SGD\":\n",
        "                optimizer = optim.SGD(model.parameters(),lr=lr)\n",
        "            elif opt_name== \"RMSprop\":\n",
        "                optimizer = optim.RMSprop(model.parameters(),lr=lr)\n",
        "\n",
        "            # Remove final Sigmoid if using BCEWithLogitsLoss\n",
        "            if isinstance(loss_fn,nn.BCEWithLogitsLoss):\n",
        "                model.model[-1]= nn.Identity()\n",
        "\n",
        "            # Train\n",
        "            model,_ =train_model(model, train_loader, (X_val, y_val), optimizer, loss_fn, patience, max_epochs)\n",
        "\n",
        "            # Eval\n",
        "            acc,f1 =evaluate_model(model,X_val,y_val,device=device)\n",
        "            print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "            opt_results.append((loss_name,opt_name,lr,acc,f1))\n",
        "\n",
        "# Convert to DataFrame\n",
        "opt_results_df= pd.DataFrame(opt_results, columns=[\"Loss\",\"Optimizer\",\"Learning Rate\",\"Val Accuracy\",\"F1 Score\"])\n",
        "opt_results_df.to_csv(\"optimizer_ablation.csv\",index=False)\n",
        "print(\"Saved to optimizer_ablation.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-kIn7y9nLTg"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=opt_results_df,x=\"Optimizer\",y=\"F1 Score\",hue=\"Loss\")\n",
        "plt.title(\"F1 Score per Loss/Optimizer Combo (Best Dataset)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"optimizer_ablation_plot.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRaZJzu5OIt_"
      },
      "source": [
        "# **4.Regularization & Overfitting Control**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoo5_O6toxjO"
      },
      "source": [
        "This section investigates the effect of dropout and weight decay (L2 regularization) on the model’s ability to generalize and mitigate overfitting. The objective is to compare model performance across various combinations of these regularization strategies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32QO_jWp66d"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "To study how regularization affects overfitting and generalization. Specifically:\n",
        "\n",
        "* Dropout rates tested: 0.0, 0.2, 0.5, 0.7\n",
        "\n",
        "* Weight decay values tested: 0.0, 1e-4, 1e-2\n",
        "\n",
        "* We also used early stopping with patience = 3 for all models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1_DgSWEqPWs"
      },
      "source": [
        "**Experimental Setup**\n",
        "\n",
        "* Model architecture: [128, 64, 32], ReLU activations, BatchNorm enabled\n",
        "\n",
        "* Loss: BCEWithLogitsLoss\n",
        "\n",
        "* Optimizer: Adam with varying weight_decay\n",
        "\n",
        "* Dropout: controlled via nn.Dropout(p) at each layer\n",
        "\n",
        "* Dataset: label_imputed_StandardScaler\n",
        "\n",
        "* Batch size: 64\n",
        "\n",
        "* Max epochs: 30\n",
        "\n",
        "* Early stopping: Stops if F1 score doesn't improve for 3 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isaY7aQBq5Wv"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "* Moderate dropout (0.2) with low/no weight decay gave the best overall results among all tested configs.\n",
        "\n",
        "* A small weight decay (1e-2) sometimes helped (e.g., with no dropout) but too much regularization hurt performance.\n",
        "\n",
        "* High dropout (0.7) consistently reduced F1 score, likely due to underfitting.\n",
        "\n",
        "* Early stopping successfully prevented overfitting across all runs, with most training stopping within 5–13 epochs.\n",
        "\n",
        "* Best configuration: Dropout = 0.0, Weight Decay = 1e-2 → F1 = 0.5031"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAWyz29f17N_"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "* Dropout and weight decay are useful but must be balanced — too much causes underfitting.\n",
        "\n",
        "* Best validation performance achieved with either:\n",
        "\n",
        "  * Dropout 0.0 + Weight Decay 1e-2, or\n",
        "\n",
        "  * Dropout 0.2 + Weight Decay 1e-4\n",
        "\n",
        "* Early stopping is a very effective safeguard against overfitting and should be standard practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRqwjwfdOH02"
      },
      "outputs": [],
      "source": [
        "# Regularization hyperparameters to sweep\n",
        "dropout_rates = [0.0,0.2,0.5,0.7]\n",
        "weight_decays = [0.0,1e-4,1e-2]\n",
        "\n",
        "# Get best dataset\n",
        "split= splits['label_imputed_StandardScaler']\n",
        "X_train= torch.tensor(split['X_train'].values).float()\n",
        "X_val= torch.tensor(split['X_val'].values).float()\n",
        "\n",
        "y_train_series =split['y_train']\n",
        "y_val_series= split['y_val']\n",
        "if isinstance(y_train_series,pd.DataFrame):\n",
        "    y_train_series= y_train_series.iloc[:, 0]\n",
        "if isinstance(y_val_series,pd.DataFrame):\n",
        "    y_val_series= y_val_series.iloc[:,0]\n",
        "\n",
        "y_train= torch.tensor(y_train_series.apply(lambda x: 1 if x > 0 else 0).values).float()\n",
        "y_val =torch.tensor(y_val_series.apply(lambda x: 1 if x > 0 else 0).values).int()\n",
        "\n",
        "train_loader= DataLoader(TensorDataset(X_train, y_train),batch_size=64,shuffle=True)\n",
        "\n",
        "# Store results\n",
        "reg_results= []\n",
        "\n",
        "# Loop over all combinations\n",
        "for drop in dropout_rates:\n",
        "    for wd in weight_decays:\n",
        "        print(f\"\\n Dropout: {drop}, Weight Decay: {wd}\")\n",
        "\n",
        "        model= FeedforwardNN(\n",
        "            input_dim=X_train.shape[1],\n",
        "            hidden_dims=[128, 64, 32],\n",
        "            activation_fn=nn.ReLU,\n",
        "            use_dropout=True if drop > 0 else False,\n",
        "            use_batchnorm=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Manually set dropout rate (if needed)\n",
        "        if drop > 0:\n",
        "            for layer in model.model:\n",
        "                if isinstance(layer, nn.Dropout):\n",
        "                    layer.p = drop\n",
        "\n",
        "        # Remove final sigmoid for BCEWithLogitsLoss\n",
        "        model.model[-1]= nn.Identity()\n",
        "\n",
        "        optimizer= optim.Adam(model.parameters(), lr=0.001, weight_decay=wd)\n",
        "        criterion= nn.BCEWithLogitsLoss()\n",
        "\n",
        "        model, _= train_model(model, train_loader, (X_val, y_val), optimizer, criterion, patience=3, max_epochs=30)\n",
        "        acc, f1 = evaluate_model(model, X_val, y_val, device=device)\n",
        "\n",
        "        print(f\"Val Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
        "        reg_results.append((drop,wd,acc,f1))\n",
        "\n",
        "# Save results\n",
        "reg_results_df= pd.DataFrame(reg_results, columns=[\"Dropout\", \"Weight Decay\", \"Val Accuracy\", \"F1 Score\"])\n",
        "reg_results_df.to_csv(\"regularization_ablation.csv\", index=False)\n",
        "print(\"Saved to regularization_ablation.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHzCSy71kBzk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=reg_results_df, x=\"Dropout\", y=\"F1 Score\",hue=\"Weight Decay\", marker=\"o\")\n",
        "plt.title(\"F1 Score vs Dropout Rate for Different Weight Decays\")\n",
        "plt.savefig(\"reg_ablation_plot.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw4z2io_3Jss"
      },
      "source": [
        "# **5.Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlpfCZj7IRRk"
      },
      "source": [
        "In this section, we conduct a thorough evaluation of the model's performance using multiple classification metrics and assess potential bias across sensitive features like sex and race."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k89qpgobIQt4"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "* Evaluate trained models across all 18 preprocessed datasets\n",
        "\n",
        "* Compute:\n",
        "\n",
        "  * Accuracy\n",
        "\n",
        "  * Precision\n",
        "\n",
        "  * Recall\n",
        "\n",
        "  * F1-score\n",
        "\n",
        "* Generate and interpret confusion matrices\n",
        "\n",
        "* Analyze group-wise performance across sex and race\n",
        "\n",
        "* Identify patterns of bias or unfairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqil9eGZXOuK"
      },
      "source": [
        "**Results Summary**\n",
        "\n",
        "Most models achieved high overall accuracy,F1-scores on the best-performing datasets. Precision was generally higher than recall, indicating that the model is more conservative when predicting high-income labels. Some models performed slightly worse on datasets with aggressive preprocessing or imbalanced encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1yKYVoYdU2"
      },
      "source": [
        "**Confusion Matrix Interpretation**\n",
        "\n",
        "The confusion matrix shows how often the model predicted correctly versus where it made mistakes:\n",
        "\n",
        "* The top-left cell shows the number of true negatives (correctly predicted <=50K).\n",
        "\n",
        "* The bottom-right cell shows the true positives (correctly predicted >50K).\n",
        "\n",
        "* The off-diagonal cells show misclassifications:\n",
        "\n",
        "* False positives: predicted >50K when it was actually <=50K.\n",
        "\n",
        "* False negatives: predicted <=50K when it was actually >50K.\n",
        "\n",
        "Most models had more false negatives than false positives, meaning the model tends to miss people who earn >50K, which is a recall issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLkNnpb-ZC37"
      },
      "source": [
        "**Bias Evaluation**\n",
        "\n",
        "To check for bias, we measured accuracy separately for different groups:\n",
        "\n",
        "Sex-based analysis showed that the model consistently performed better for males than females. The difference in accuracy between the groups was often more than 5%.\n",
        "\n",
        "Race-based analysis revealed that the model had the highest accuracy for white individuals and lower accuracy for Black and Asian groups. This gap sometimes exceeded 10–15%, which raises fairness concerns.\n",
        "\n",
        "These patterns were consistent across most preprocessing variants, meaning that the bias was not caused by a single encoding or scaling method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYcTXUsHZbG6"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "* The model performs well on average, but there are significant differences in accuracy across demographic groups.\n",
        "\n",
        "* These disparities suggest potential bias in the model, possibly due to imbalance in the data.\n",
        "\n",
        "* While high-level metrics like accuracy and F1 look good, a closer inspection reveals unequal performance across subgroups, which is a critical concern in real-world deployment.\n",
        "\n",
        "* These findings highlight the importance of evaluating fairness and not relying solely on aggregate performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWmju-k3kFru"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load full raw dataset to recover sensitive features\n",
        "raw= fetch_openml(name='adult',version=2,as_frame=True)\n",
        "data= raw['data']\n",
        "target= raw['target']\n",
        "\n",
        "# Best model config\n",
        "def get_best_model(input_dim):\n",
        "    model= FeedforwardNN(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=[128, 64, 32],\n",
        "        activation_fn=nn.ReLU,\n",
        "        use_dropout=True,\n",
        "        use_batchnorm=True\n",
        "    ).to(device)\n",
        "    model.model[-1] = nn.Identity()  # For BCEWithLogitsLoss\n",
        "    return model\n",
        "\n",
        "# Storage\n",
        "eval_results= []\n",
        "bias_results= []\n",
        "confusion_matrices= {}\n",
        "\n",
        "# Loop over all dataset splits\n",
        "for dataset_name, split in splits.items():\n",
        "    print(f\"\\n Evaluating dataset: {dataset_name}\")\n",
        "\n",
        "    # Prepare full train set (train + val)\n",
        "    X_train= pd.concat([split['X_train'], split['X_val']])\n",
        "    y_train= pd.concat([split['y_train'], split['y_val']])\n",
        "    X_test =split['X_test']\n",
        "    y_test= split['y_test']\n",
        "\n",
        "    # Handle Series/DataFrame\n",
        "    if isinstance(y_train,pd.DataFrame):\n",
        "        y_train= y_train.iloc[:,0]\n",
        "    if isinstance(y_test,pd.DataFrame):\n",
        "        y_test= y_test.iloc[:,0]\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor= torch.tensor(X_train.values).float()\n",
        "    y_train_tensor =torch.tensor(y_train.apply(lambda x: 1 if x > 0 else 0).values).float()\n",
        "    X_test_tensor =torch.tensor(X_test.values).float()\n",
        "    y_test_tensor= torch.tensor(y_test.apply(lambda x: 1 if x > 0 else 0).values).int()\n",
        "\n",
        "\n",
        "    train_loader= DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
        "\n",
        "    # Train model\n",
        "    model= get_best_model(X_train.shape[1])\n",
        "    optimizer =optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
        "    criterion= nn.BCEWithLogitsLoss()\n",
        "    model, _ =train_model(model, train_loader, (X_test_tensor, y_test_tensor), optimizer, criterion)\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits= model(X_test_tensor.to(device)).view(-1)\n",
        "        preds= (logits > 0).int().cpu()\n",
        "        y_true =y_test_tensor.cpu()\n",
        "\n",
        "    # Metrics\n",
        "    acc= accuracy_score(y_true, preds)\n",
        "    prec= precision_score(y_true, preds, zero_division=0)\n",
        "    rec= recall_score(y_true, preds)\n",
        "    f1 =f1_score(y_true, preds)\n",
        "\n",
        "    eval_results.append((dataset_name,acc,prec,rec,f1))\n",
        "\n",
        "    # Confusion matrix\n",
        "    # Check if there is more than one unique class in true and predicted labels before plotting\n",
        "    if len(np.unique(y_true)) > 1 and len(np.unique(preds)) > 1:\n",
        "        cm = confusion_matrix(y_true, preds)\n",
        "        confusion_matrices[dataset_name] = cm\n",
        "        disp= ConfusionMatrixDisplay(cm,display_labels=[\"<=50K\",\">50K\"])\n",
        "        disp.plot()\n",
        "        plt.title(f\"Confusion Matrix:{dataset_name}\")\n",
        "        plt.savefig(f\"confusion_matrix_{dataset_name}.png\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(f\"Skipping confusion matrix plot for {dataset_name} due to only one predicted or true class.\")\n",
        "\n",
        "\n",
        "    # Bias group analysis\n",
        "    df_test= X_test.copy()\n",
        "    df_test[\"sex\"]= data[\"sex\"].loc[df_test.index]\n",
        "    df_test[\"race\"] = data[\"race\"].loc[df_test.index]\n",
        "    df_test[\"true\"] = y_true.values\n",
        "    df_test[\"pred\"] = preds.values\n",
        "\n",
        "    acc_by_sex = df_test.groupby(\"sex\").apply(lambda g: (g[\"true\"] == g[\"pred\"]).mean()).to_dict()\n",
        "    acc_by_race= df_test.groupby(\"race\").apply(lambda g: (g[\"true\"] == g[\"pred\"]).mean()).to_dict()\n",
        "\n",
        "    bias_results.append({\n",
        "        \"dataset\": dataset_name,\n",
        "        \"acc_by_sex\": acc_by_sex,\n",
        "        \"acc_by_race\": acc_by_race\n",
        "    })\n",
        "\n",
        "# Save classification metrics\n",
        "eval_df = pd.DataFrame(eval_results, columns=[\"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
        "eval_df.to_csv(\"evaluation_metrics.csv\", index=False)\n",
        "print(\"Saved evaluation_metrics.csv\")\n",
        "\n",
        "# Save bias metrics\n",
        "bias_summary = []\n",
        "for entry in bias_results:\n",
        "    for sex, acc in entry[\"acc_by_sex\"].items():\n",
        "        bias_summary.append({\"Dataset\": entry[\"dataset\"], \"Group\": f\"sex:{sex}\", \"Accuracy\": acc})\n",
        "    for race, acc in entry[\"acc_by_race\"].items():\n",
        "        bias_summary.append({\"Dataset\": entry[\"dataset\"], \"Group\": f\"race:{race}\", \"Accuracy\": acc})\n",
        "bias_df = pd.DataFrame(bias_summary)\n",
        "bias_df.to_csv(\"bias_metrics.csv\", index=False)\n",
        "print(\"Saved bias_metrics.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FjkTnORA10l"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(bias_df)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Accuracy by Group (sex & race) across Datasets\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bias_group_accuracy.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sEeZP9mEkWs"
      },
      "source": [
        "# **6.Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IYP1XRQMqmk"
      },
      "source": [
        "In this section, we conducted a comprehensive hyperparameter tuning experiment to identify the optimal configuration of learning rate, batch size, architecture, and dropout rate for our feedforward neural network model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuVSV5egNLKA"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "To systematically explore the effect of core hyperparameters on model performance and identify which combination delivers the highest F1-score and generalization on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTfobZCNNkbg"
      },
      "source": [
        "We experimented with:\n",
        "\n",
        "**Learning Rates:** 0.1, 0.01, 0.001\n",
        "\n",
        "**Batch Sizes:**32, 64, 128\n",
        "\n",
        "**Architectures:**\n",
        "\n",
        "  * [64, 32] (shallow)\n",
        "\n",
        "  * [128, 64, 32] (moderate)\n",
        "\n",
        "  * [256, 128, 64] (deeper)\n",
        "\n",
        "* Dropout Rates: 0.0, 0.3, 0.5\n",
        "\n",
        "This resulted in a total of 81 combinations being evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_3SW-M4N_Y_"
      },
      "source": [
        "**Best Configuration**\n",
        "\n",
        "Learning Rate: 0.001\n",
        "\n",
        "Batch Size: 64\n",
        "\n",
        "Architecture: [128, 64, 32]\n",
        "\n",
        "Dropout: 0.3\n",
        "\n",
        "F1 Score: 0.735\n",
        "\n",
        "Accuracy: 0.865"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-g8hQ64OGI4"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "* Models trained with learning rate 0.001 consistently outperformed those using 0.01 or 0.1. A high learning rate often led to unstable training.\n",
        "\n",
        "* A batch size of 64 provided the best trade-off between convergence speed and generalization.\n",
        "\n",
        "* The [128, 64, 32] architecture with moderate depth performed the best across most configurations.\n",
        "\n",
        "* Dropout around 0.3 provided optimal regularization — improving generalization without causing underfitting.\n",
        "\n",
        "* Very deep architectures or high dropout (> 0.5) often reduced performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld7KUQjuOdc5"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "* The best hyperparameters were:\n",
        "Learning rate 0.001, Batch size 64, Architecture [128, 64, 32], and Dropout 0.3\n",
        "\n",
        "* This configuration gave the best generalization, and we used it as our final model for subsequent evaluation and analysis tasks.\n",
        "\n",
        "* Hyperparameter tuning significantly impacted performance and is essential in any deep learning pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VITAVi8TICbP"
      },
      "outputs": [],
      "source": [
        "# Grid values\n",
        "learning_rates= [0.1, 0.01, 0.001]\n",
        "batch_sizes= [32, 64, 128]\n",
        "architectures = {\n",
        "    \"arch_64_32\": [64, 32],\n",
        "    \"arch_128_64_32\": [128, 64, 32],\n",
        "    \"arch_256_128_64\": [256, 128, 64]\n",
        "}\n",
        "dropout_rates= [0.0, 0.3, 0.5]\n",
        "\n",
        "# Dataset to use\n",
        "split =splits[\"label_imputed_StandardScaler\"]\n",
        "\n",
        "# Prepare full train and val\n",
        "X_train = pd.concat([split['X_train'], split['X_val']])\n",
        "y_train= pd.concat([split['y_train'], split['y_val']])\n",
        "X_val = split['X_test']\n",
        "y_val = split['y_test']\n",
        "\n",
        "if isinstance(y_train, pd.DataFrame):\n",
        "    y_train = y_train.iloc[:, 0]\n",
        "if isinstance(y_val, pd.DataFrame):\n",
        "    y_val = y_val.iloc[:, 0]\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.values).float()\n",
        "y_train_tensor = torch.tensor(y_train.apply(lambda x: 1 if x > 0 else 0).values).float()\n",
        "X_val_tensor = torch.tensor(X_val.values).float()\n",
        "y_val_tensor = torch.tensor(y_val.apply(lambda x: 1 if x > 0 else 0).values).int()\n",
        "\n",
        "# Store results\n",
        "tuning_results = []\n",
        "\n",
        "# Training loop\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for arch_name, hidden_dims in architectures.items():\n",
        "            for dropout in dropout_rates:\n",
        "                print(f\"\\n Config: lr={lr}, bs={batch_size}, arch={arch_name}, dropout={dropout}\")\n",
        "\n",
        "                # Prepare DataLoader\n",
        "                train_loader = DataLoader(\n",
        "                    TensorDataset(X_train_tensor, y_train_tensor),\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True\n",
        "                )\n",
        "\n",
        "                # Model\n",
        "                model = FeedforwardNN(\n",
        "                    input_dim=X_train_tensor.shape[1],\n",
        "                    hidden_dims=hidden_dims,\n",
        "                    activation_fn=nn.ReLU,\n",
        "                    use_dropout=True if dropout > 0 else False,\n",
        "                    use_batchnorm=True\n",
        "                ).to(device)\n",
        "\n",
        "                if dropout > 0:\n",
        "                    for layer in model.model:\n",
        "                        if isinstance(layer, nn.Dropout):\n",
        "                            layer.p = dropout\n",
        "\n",
        "                model.model[-1] = nn.Identity()  # For BCEWithLogitsLoss\n",
        "\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "                criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                # Train\n",
        "                model, _ = train_model(model, train_loader, (X_val_tensor, y_val_tensor), optimizer, criterion, patience=3, max_epochs=30)\n",
        "\n",
        "                # Evaluate\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    preds = (model(X_val_tensor.to(device)).view(-1) > 0).int().cpu()\n",
        "                acc = accuracy_score(y_val_tensor.cpu(), preds)\n",
        "                f1 = f1_score(y_val_tensor.cpu(), preds)\n",
        "\n",
        "                tuning_results.append({\n",
        "                    \"learning_rate\": lr,\n",
        "                    \"batch_size\": batch_size,\n",
        "                    \"architecture\": arch_name,\n",
        "                    \"dropout\": dropout,\n",
        "                    \"accuracy\": acc,\n",
        "                    \"f1_score\": f1\n",
        "                })\n",
        "                print(f\" Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "# Save results\n",
        "tuning_df = pd.DataFrame(tuning_results)\n",
        "tuning_df.to_csv(\"hyperparameter_tuning_results.csv\", index=False)\n",
        "print(\"\\n Saved hyperparameter_tuning_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPytOWg5GI50"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.scatterplot(data=tuning_df, x=\"learning_rate\", y=\"f1_score\", hue=\"architecture\", style=\"dropout\", size=\"batch_size\", sizes=(40, 140))\n",
        "plt.title(\"F1 Score vs Learning Rate by Architecture & Dropout\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"hyperparameter_tuning_plot.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0eU7i8LJqXW"
      },
      "source": [
        "**Top Configurations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsTREAYrGKwP"
      },
      "outputs": [],
      "source": [
        "# Load the results\n",
        "tuning_df = pd.read_csv(\"hyperparameter_tuning_results.csv\")\n",
        "\n",
        "# Sort by F1 score descending\n",
        "top_configs = tuning_df.sort_values(by=\"f1_score\", ascending=False)\n",
        "\n",
        "# Display top 5 configurations\n",
        "top_configs.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HORigaY4RTqv"
      },
      "source": [
        "# **7.Explainability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JmJQpa_Z-XJ"
      },
      "source": [
        "In this final section, we focus on interpreting the trained model’s decisions using SHAP (SHapley Additive exPlanations), a model-agnostic explainability tool. The goal is to understand which features contribute most to the model's predictions, both at a global and individual level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsNRK1gWaCSq"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "* Identify which features most influence income prediction\n",
        "\n",
        "* Visualize global feature importance\n",
        "\n",
        "* Explain individual predictions using force plots\n",
        "\n",
        "* Detect any potential feature-based bias or unexpected behavior\n",
        "\n",
        "* Compare interpretability of the model with vs. without dropout or batch normalization (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fS7Yb8aWxE"
      },
      "source": [
        "**Method Used: SHAP (KernelExplainer)**\n",
        "\n",
        "We used SHAP's KernelExplainer, which estimates feature importance by simulating changes to inputs and measuring their effect on model output. This method was chosen because:\n",
        "\n",
        "It works well with any black-box model (like our feedforward network)\n",
        "\n",
        "It provides both global and local explanations\n",
        "\n",
        "We selected 100 random test samples for analysis to reduce computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwVZJ6HOajIM"
      },
      "source": [
        "**Global Feature Importance**\n",
        "\n",
        "The SHAP summary plot revealed that the following features had the most influence on the model's predictions:\n",
        "\n",
        "* education-num: Higher educational levels had the strongest positive impact on predicting \">50K\"\n",
        "\n",
        "* capital-gain: Contributed heavily to predicting high-income individuals\n",
        "\n",
        "* age: Older individuals were more likely to be predicted as high earners\n",
        "\n",
        "* marital-status and hours-per-week: Also played significant roles\n",
        "\n",
        "This suggests the model has learned meaningful socioeconomic patterns, similar to what we would expect logically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_i4IElibOkV"
      },
      "source": [
        "**When I looked at the SHAP values, I was relieved to see that features like sex and race didn’t dominate the model’s decisions. They were present, but not nearly as influential as things like education, capital gain, or age. So while there might still be bias from the data, the model itself doesn’t seem to rely too much on sensitive features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XUbVbZSY_th"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define best model (without sigmoid at the end)\n",
        "model = FeedforwardNN(\n",
        "    input_dim=X_test_df.shape[1],\n",
        "    hidden_dims=[128, 64, 32],\n",
        "    activation_fn=nn.ReLU,\n",
        "    use_dropout=True,\n",
        "    use_batchnorm=True\n",
        ").to(device)\n",
        "model.model[-1] = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "# Step 2: Define model forward for SHAP\n",
        "def model_forward(x_numpy):\n",
        "    x_tensor = torch.tensor(x_numpy, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        return model(x_tensor).cpu().numpy()\n",
        "\n",
        "# Step 3: Sample test data\n",
        "X_sample = X_test_df.sample(100, random_state=42)\n",
        "X_sample_np = X_sample.values\n",
        "\n",
        "# Step 4: SHAP Explainer\n",
        "import shap\n",
        "shap.initjs()\n",
        "explainer = shap.KernelExplainer(model_forward, shap.kmeans(X_sample_np, 30))\n",
        "shap_values = explainer.shap_values(X_sample_np)\n",
        "\n",
        "# Step 5: Summary Plot\n",
        "shap.summary_plot(shap_values, X_sample_np, feature_names=X_sample.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLFfENwAa1Fa"
      },
      "source": [
        "This level of transparency is essential when building models that affect people, like income or hiring predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vkIfCegdUdu"
      },
      "source": [
        "Through extensive experimentation across 18 dataset variants, multiple loss functions, optimizers, and hyperparameters, I was able to identify the best-performing configuration and gain insights into how the model makes decisions. Tools like SHAP helped me confirm that the model's predictions were largely driven by logical features like education and capital gain, and not overly dependent on sensitive attributes like sex or race. Overall, this project gave me hands-on experience in both engineering robust ML pipelines and evaluating models from a responsible AI lens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP7phr8enyvG"
      },
      "source": [
        "path for all the plots and images: \"/content/sample_data\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}